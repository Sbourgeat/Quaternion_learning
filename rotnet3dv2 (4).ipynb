{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f08331-1454-48ec-bc3e-f251f427c6ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T12:41:58.182406Z",
     "iopub.status.busy": "2024-08-23T12:41:58.180916Z",
     "iopub.status.idle": "2024-08-23T12:41:58.197451Z",
     "shell.execute_reply": "2024-08-23T12:41:58.195491Z",
     "shell.execute_reply.started": "2024-08-23T12:41:58.182299Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d9d8cd5-7a32-4fce-a2c0-7346d2789f96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T12:41:58.825074Z",
     "iopub.status.busy": "2024-08-23T12:41:58.824190Z",
     "iopub.status.idle": "2024-08-23T12:41:58.843329Z",
     "shell.execute_reply": "2024-08-23T12:41:58.841216Z",
     "shell.execute_reply.started": "2024-08-23T12:41:58.824999Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_images(directory_path, target_shape):\n",
    "    images = []\n",
    "    target_shape = (target_shape, target_shape, target_shape)\n",
    "    for filename in sorted(os.listdir(directory_path)):\n",
    "        if filename.endswith(\".tif\"):\n",
    "            image = tiff.imread(os.path.join(directory_path, filename))\n",
    "            if image.shape != target_shape:\n",
    "                image = resize(\n",
    "                    image, target_shape, preserve_range=True, anti_aliasing=True\n",
    "                )\n",
    "                image = image.astype(np.float32)\n",
    "            images.append(image)\n",
    "    images = np.array(images)\n",
    "    images = np.expand_dims(images, axis=-1)  # Add channel dimension\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1492effb-80dc-4392-a690-82e8109f1fb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T12:41:59.206703Z",
     "iopub.status.busy": "2024-08-23T12:41:59.205894Z",
     "iopub.status.idle": "2024-08-23T12:41:59.221876Z",
     "shell.execute_reply": "2024-08-23T12:41:59.219147Z",
     "shell.execute_reply.started": "2024-08-23T12:41:59.206633Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize(source_images):\n",
    "    normalized_images = []\n",
    "    for img in source_images:\n",
    "        min_val = np.min(img)\n",
    "        max_val = np.max(img)\n",
    "        normalized_img = (img - min_val) / (max_val - min_val + np.finfo(float).eps)\n",
    "        normalized_images.append(normalized_img)\n",
    "    normalized_images = np.array(normalized_images)\n",
    "    normalized_images = np.clip(normalized_images, 0, 1)\n",
    "    normalized_images = normalized_images.astype(np.float32)\n",
    "    return normalized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3aaee99-a631-48a7-9760-404a2461f54e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T12:44:03.844604Z",
     "iopub.status.busy": "2024-08-23T12:44:03.843705Z",
     "iopub.status.idle": "2024-08-23T12:44:03.861466Z",
     "shell.execute_reply": "2024-08-23T12:44:03.859097Z",
     "shell.execute_reply.started": "2024-08-23T12:44:03.844522Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def binarize_targets(target_path, threshold = 0.1):\n",
    "    binarized_image = []\n",
    "    for targets in target_path:\n",
    "        targets[targets >= threshold] = 1\n",
    "        targets[targets < threshold] = 0\n",
    "        binarized_image.append(targets)\n",
    "\n",
    "    targets = np.array(binarized_image)\n",
    "    targets = np.clip(targets, 0, 1)\n",
    "    targets = targets.astype(np.float32)\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61cb1f1-736e-4c6e-93c9-1821bbfd21ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quat_finder(images):\n",
    "    quaternions = []\n",
    "    for image in images:\n",
    "        \n",
    "        # Step 1: Extract coordinates of non-zero voxels\n",
    "        activated_coords = np.array(np.where(image > 0)).T\n",
    "\n",
    "        # Step 2: Perform PCA\n",
    "        pca = PCA(n_components=3)\n",
    "        pca.fit(activated_coords)\n",
    "\n",
    "        # Get the principal components\n",
    "        principal_components = pca.components_\n",
    "\n",
    "        # Step 3: Use the principal components to define a rotation quaternion\n",
    "        rotation_matrix = principal_components.T\n",
    "        rotation_quaternion = R.from_matrix(rotation_matrix).as_quat()\n",
    "        quaternions.append(rotation_quaternion)\n",
    "    \n",
    "    quaternions = torch.from_numpy(np.array(quaternions))\n",
    "\n",
    "\n",
    "    return quaternions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd68e86-e6e7-4990-977f-88a468eeb5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, source_images, downsampling_factor):\n",
    "        self.source_images = torch.tensor(source_images, dtype=torch.float32)\n",
    "        self.source_images = self.source_images.permute(0, 4, 2, 3, 1)\n",
    "        self.source_images = torch.nn.functional.interpolate(\n",
    "            self.source_images, scale_factor=1 / downsampling_factor\n",
    "        )\n",
    "        self.source_images = self.source_images.permute(0, 2, 3, 4, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.source_images[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d44d741-4def-42ae-b2ac-5ba47882f64a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-23T13:05:26.067088Z",
     "iopub.status.busy": "2024-08-23T13:05:26.065193Z",
     "iopub.status.idle": "2024-08-23T13:05:26.198456Z",
     "shell.execute_reply": "2024-08-23T13:05:26.196810Z",
     "shell.execute_reply.started": "2024-08-23T13:05:26.066990Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 66\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Simulation d'un ensemble de données volumétriques (synthetique pour l'exemple)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# volumes_train = torch.randn(100, 1, 32, 32, 32)  # Exemple de volumes 3D de 32x32x32\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# labels_train = torch.randn(100, 4)  # Quaternions associés\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# DataLoader\u001b[39;00m\n\u001b[1;32m     65\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m---> 66\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtrain_set\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     67\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_set, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Instantiation du modèle\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "# Définition du modèle\n",
    "class OrientationNetQuaternion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OrientationNetQuaternion, self).__init__()\n",
    "        # 3D convolutional layers\n",
    "        self.conv1 = nn.Conv3d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8 * 8, 512)  # Adjust according to input size\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 4)  # Output 4 quaternions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool3d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool3d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool3d(x, 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Output the quaternion\n",
    "        return x\n",
    "\n",
    "# Fonction d'entraînement\n",
    "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for volumes, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(volumes)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Fonction de test\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for volumes, labels in test_loader:\n",
    "            outputs = model(volumes)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    print(f'Average test loss: {total_loss/len(test_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a467887f-342a-4514-95e4-b82c4569ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coded configuration\n",
    "config = Config()\n",
    "config.model_path = \"./model_3d_opti.pth\"  # Change to your model path\n",
    "config.input_dir = \"./training/source/\"  # Change to your input images directory\n",
    "config.output_dir = \"./predictions\"  # Change to your output directory\n",
    "config.batch_size = 1\n",
    "config.target_shape = 192\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model\n",
    "model = OrientationNetQuaternion()\n",
    "model.load_state_dict(torch.load(config.model_path, map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load the input images\n",
    "input_images = load_images(config.input_dir, config.target_shape)\n",
    "original_shape = input_images[0].shape\n",
    "input_images = normalize(input_images)\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = CustomDataset(input_images)\n",
    "dataloader = DataLoader(dataset, batch_size = config.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 20\n",
    "best_val_loss = float(\"inf\")\n",
    "early_stop_counter = 0\n",
    "\n",
    "# Training and evaluation loop\n",
    "def evaluate(model, dataloader, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            q_target = quat_finder(targets)\n",
    "            q_output = quat_finder(outputs)\n",
    "            loss = criterion(q_outputs, q_targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.numel()\n",
    "            dice_score += dice_coefficient(predicted, targets) * inputs.size(0)\n",
    "    val_loss /= len(dataloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    dice_score /= len(dataloader.dataset)\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": accuracy,\n",
    "            \"val_dice\": dice_score,\n",
    "        }\n",
    "    )\n",
    "    return val_loss, accuracy, dice_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0d0d2e-3a46-4cb7-8eea-3e36bc03648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config.num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with tqdm(\n",
    "        total=len(train_loader),\n",
    "        desc=f\"Epoch {epoch + 1}/{config.num_epochs}\",\n",
    "        unit=\"batch\",\n",
    "    ) as pbar:\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.numel()\n",
    "            pbar.update(1)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = correct / total\n",
    "    val_loss, val_accuracy, val_dice = evaluate(\n",
    "        model, test_loader, criterion, device, epoch\n",
    "    )\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model_3d_aug.pth\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "torch.save(model.state_dict(), \"model_3d_aug.pth\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba0336-e1d1-46ca-ad2e-08fb8a9f225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform predictions\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs in tqdm(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "# Save the predictions\n",
    "save_predictions(predictions, config.output_dir, original_shape)\n",
    "print(f\"Predictions saved to {config.output_dir}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
